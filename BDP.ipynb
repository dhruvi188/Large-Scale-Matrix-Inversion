{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Spark Implementation For Matrix Inversion"
      ],
      "metadata": {
        "id": "uK-It8xcorGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import RDD\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, ArrayType"
      ],
      "metadata": {
        "id": "-Ty7Q76Il1o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##List of functions to be used"
      ],
      "metadata": {
        "id": "pBv-6XHIow68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SparkMatrixInversion:\n",
        "    def __init__(self, spark_session: SparkSession):\n",
        "        self.spark = spark_session\n",
        "\n",
        "    def create_index_row_matrix(self, matrix: np.ndarray) -> RDD:\n",
        "        pass\n",
        "\n",
        "    def create_block_matrix(self, matrix: np.ndarray, block_size: int = 64) -> RDD:\n",
        "        pass\n",
        "\n",
        "    def block_matrix_multiplication(self, A_rdd: RDD, B_rdd: RDD, block_size: int) -> RDD:\n",
        "        pass\n",
        "\n",
        "    def array_permutation(self, matrix_rdd: RDD, perm_array: List[int]) -> RDD:\n",
        "        pass\n",
        "\n",
        "    def block_lu_decomposition(self, matrix_rdd: RDD, block_size: int = 64):\n",
        "        pass\n",
        "\n",
        "    def block_matrix_inverse(self, matrix: np.ndarray, block_size: int = 64):\n",
        "        pass\n",
        "\n",
        "    def lu_decompose(self, A):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "WxPlpRs9lHzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Index Row Matrix from Numpy Matrix"
      ],
      "metadata": {
        "id": "6qOAB_5lo2iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_index_row_matrix(self, matrix: np.ndarray) -> RDD:\n",
        "        \"\"\"\n",
        "        Convert a numpy matrix to a Spark RDD in IndexRowMatrix format\n",
        "\n",
        "        Args:\n",
        "            matrix (np.ndarray): Input matrix to be distributed\n",
        "\n",
        "        Returns:\n",
        "            RDD of matrix rows with indices\n",
        "        \"\"\"\n",
        "        rows_with_indices = []\n",
        "        for i, row in enumerate(matrix):\n",
        "            for j, val in enumerate(row):\n",
        "                rows_with_indices.append((i, j, val))\n",
        "\n",
        "        return self.spark.sparkContext.parallelize(rows_with_indices)\n",
        "SparkMatrixInversion.create_index_row_matrix = create_index_row_matrix"
      ],
      "metadata": {
        "id": "H6MZiOCQmSwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Block Matrix from numpy Matrix"
      ],
      "metadata": {
        "id": "OfD-IK7Oo-iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_block_matrix(self, matrix: np.ndarray, block_size: int = 64) -> RDD:\n",
        "        \"\"\"\n",
        "        Convert a numpy matrix to a BlockMatrix format\n",
        "\n",
        "        Args:\n",
        "            matrix (np.ndarray): Input matrix\n",
        "            block_size (int): Size of each block\n",
        "\n",
        "        Returns:\n",
        "            RDD of BlockMatrix with (block_id, block_values)\n",
        "        \"\"\"\n",
        "        blocks = []\n",
        "        n = matrix.shape[0]\n",
        "\n",
        "        for i in range(0, n, block_size):\n",
        "            for j in range(0, n, block_size):\n",
        "                block = matrix[i:i+block_size, j:j+block_size]\n",
        "                block_id = (i // block_size, j // block_size)\n",
        "                blocks.append((block_id, block))\n",
        "\n",
        "        return self.spark.sparkContext.parallelize(blocks)\n",
        "SparkMatrixInversion.create_block_matrix = create_block_matrix"
      ],
      "metadata": {
        "id": "F9ednN6enANF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spark optimized 1-D Block based Matrix Multiplication"
      ],
      "metadata": {
        "id": "zPigyLKUpBgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def block_matrix_multiplication(self, A_rdd: RDD, B_rdd: RDD, block_size: int) -> RDD:\n",
        "        \"\"\"\n",
        "        1-D block-based matrix multiplication\n",
        "\n",
        "        Args:\n",
        "            A_rdd (RDD): First matrix RDD\n",
        "            B_rdd (RDD): Second matrix RDD\n",
        "            block_size (int): Size of blocks\n",
        "\n",
        "        Returns:\n",
        "            RDD of multiplied matrix blocks\n",
        "        \"\"\"\n",
        "        flat_A = A_rdd.flatMap(lambda x: [(x[0][0] * len(x[1]) + x[0][1], x[1])])\n",
        "        flat_B = B_rdd.flatMap(lambda x: [(x[0][1] * len(x[1]) + x[0][0], x[1].T)])\n",
        "\n",
        "        flat_AB_pair = flat_A.join(flat_B)\n",
        "\n",
        "        result = flat_AB_pair.map(lambda x: (\n",
        "            (x[1][0].shape[0], x[1][1].shape[1]),\n",
        "            np.dot(x[1][0], x[1][1])\n",
        "        ))\n",
        "        return result.reduceByKey(lambda a, b: a + b)\n",
        "SparkMatrixInversion.block_matrix_multiplication = block_matrix_multiplication"
      ],
      "metadata": {
        "id": "mdCtNjEOnR_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spark based optimized permutation (Could not use)"
      ],
      "metadata": {
        "id": "XOSlRvizprS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def array_permutation(self, matrix_rdd: RDD, perm_array: List[int]) -> RDD:\n",
        "        \"\"\"\n",
        "        Perform row permutation on block-based matrix\n",
        "\n",
        "        Args:\n",
        "            matrix_rdd (RDD): Block-based matrix\n",
        "            perm_array (List[int]): Permutation array\n",
        "\n",
        "        Returns:\n",
        "            Permuted matrix RDD\n",
        "        \"\"\"\n",
        "        def flatmap_permute(block):\n",
        "            block_id, block_data = block\n",
        "            block_rows, block_cols = block_data.shape\n",
        "\n",
        "            permuted_rows = []\n",
        "            for local_row in range(block_rows):\n",
        "                global_row = block_id[0] * block_rows + local_row\n",
        "                new_global_row = perm_array.index(global_row)\n",
        "\n",
        "                new_block_row = new_global_row // block_rows\n",
        "                new_local_row = new_global_row % block_rows\n",
        "\n",
        "                new_block_id = (new_block_row, block_id[1])\n",
        "                permuted_rows.append((new_block_id, block_data[local_row, :]))\n",
        "\n",
        "            return permuted_rows\n",
        "\n",
        "        return matrix_rdd.flatMap(flatmap_permute).groupByKey().mapValues(list)\n",
        "SparkMatrixInversion.array_permutation = array_permutation"
      ],
      "metadata": {
        "id": "QkRgSThGnYhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Recursive Function for LU Decomposition"
      ],
      "metadata": {
        "id": "yiG7rcCMpy31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def block_lu_decomposition(self, matrix_rdd: RDD, block_size: int = 64):\n",
        "        \"\"\"\n",
        "        Perform Block LU Decomposition on distributed matrix using Spark\n",
        "\n",
        "        Args:\n",
        "            matrix_rdd (RDD): Block-based matrix RDD\n",
        "            block_size (int): Size of blocks\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing LU decomposition components\n",
        "        \"\"\"\n",
        "        matrix_blocks = dict(matrix_rdd.collect())\n",
        "\n",
        "        max_row_block = max(block[0][0] for block in matrix_blocks.items()) + 1\n",
        "        max_col_block = max(block[0][1] for block in matrix_blocks.items()) + 1\n",
        "        n = max_row_block * block_size\n",
        "\n",
        "        M = np.zeros((n, n))\n",
        "        for (i, j), block in matrix_blocks.items():\n",
        "            row_start = i * block_size\n",
        "            col_start = j * block_size\n",
        "            block_height, block_width = block.shape\n",
        "            M[row_start:row_start+block_height, col_start:col_start+block_width] = block\n",
        "\n",
        "        def recursive_block_lu(M):\n",
        "            n = M.shape[0]\n",
        "            mid = n // 2\n",
        "\n",
        "            if mid <= 64:\n",
        "                L1, U1, P1 = self.lu_decompose(M)\n",
        "                L1_inv = np.linalg.inv(L1)\n",
        "                U1_inv = np.linalg.inv(U1)\n",
        "                T = U1_inv @ L1_inv\n",
        "\n",
        "                return {\n",
        "                    'L1_inv': L1_inv,\n",
        "                    'U1_inv': U1_inv,\n",
        "                    'P1': P1,\n",
        "                    'T': T\n",
        "                }\n",
        "\n",
        "            M1 = M[:mid, :mid]\n",
        "            M2 = M[:mid, mid:]\n",
        "            M3 = M[mid:, :mid]\n",
        "            M4 = M[mid:, mid:]\n",
        "\n",
        "            M1_lu = recursive_block_lu(M1)\n",
        "            L1_inv = M1_lu['L1_inv']\n",
        "            U1_inv = M1_lu['U1_inv']\n",
        "            P1 = M1_lu['P1']\n",
        "            T = M1_lu['T']\n",
        "\n",
        "            S_hat = M3 @ T\n",
        "\n",
        "            M_hat = M4 - S_hat @ (P1 @ M2)\n",
        "\n",
        "            M_hat_lu = recursive_block_lu(M_hat)\n",
        "\n",
        "            return {\n",
        "                'L1_inv': L1_inv,\n",
        "                'S_hat': S_hat,\n",
        "                'L3_inv': M_hat_lu['L1_inv'],\n",
        "                'U1_inv': U1_inv,\n",
        "                'T': T,\n",
        "                'P1M2': P1 @ M2,\n",
        "                'U3_inv': M_hat_lu['U1_inv'],\n",
        "                'P1': P1,\n",
        "                'P2': M_hat_lu['P1']\n",
        "            }\n",
        "\n",
        "        lu_result = recursive_block_lu(M)\n",
        "        return lu_result\n",
        "SparkMatrixInversion.block_lu_decomposition = block_lu_decomposition"
      ],
      "metadata": {
        "id": "wPw3U3NHnkP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function that takes numpy array as input and returns it's Inverse"
      ],
      "metadata": {
        "id": "SJsOtywfp2m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def block_matrix_inverse(self, matrix: np.ndarray, block_size: int = 64):\n",
        "        \"\"\"\n",
        "        Compute matrix inverse using block-based approach with Spark\n",
        "\n",
        "        Args:\n",
        "            matrix (np.ndarray): Input matrix\n",
        "            block_size (int): Size of blocks\n",
        "\n",
        "        Returns:\n",
        "            Inverted matrix\n",
        "        \"\"\"\n",
        "        # Convert to block matrix RDD\n",
        "        block_matrix = self.create_block_matrix(matrix, block_size)\n",
        "\n",
        "        # Perform block LU decomposition\n",
        "        lu_decomp = self.block_lu_decomposition(block_matrix, block_size)\n",
        "\n",
        "        # Extract LU decomposition components\n",
        "        L1_inv = lu_decomp['L1_inv']\n",
        "        S_hat = lu_decomp.get('S_hat', np.zeros_like(L1_inv))\n",
        "        L3_inv = lu_decomp['L3_inv']\n",
        "        U1_inv = lu_decomp['U1_inv']\n",
        "        T = lu_decomp['T']\n",
        "        P1A2 = lu_decomp['P1M2']\n",
        "        U3_inv = lu_decomp['U3_inv']\n",
        "        P1 = lu_decomp['P1']\n",
        "        P2 = lu_decomp['P2']\n",
        "\n",
        "        # Compute inverse blocks\n",
        "        n = matrix.shape[0]\n",
        "        mid = n // 2\n",
        "\n",
        "        X1 = T @ P1\n",
        "        X2 = U3_inv @ L3_inv\n",
        "        Y1 = X1 @ matrix[:mid, mid:]\n",
        "        Y2 = X2 @ matrix[mid:, :mid]\n",
        "\n",
        "        A_inv_4 = X2\n",
        "        A_inv_3 = -Y2 @ X1\n",
        "        A_inv_2 = -Y1 @ X2\n",
        "        A_inv_1 = X1 - Y1 @ A_inv_3\n",
        "\n",
        "        # Reconstruct inverse matrix\n",
        "        A_inv = np.block([[A_inv_1, A_inv_2],\n",
        "                        [A_inv_3, A_inv_4]])\n",
        "\n",
        "        return A_inv\n",
        "SparkMatrixInversion.block_matrix_inverse = block_matrix_inverse"
      ],
      "metadata": {
        "id": "6qBcR4rXnv2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basic function for LU Decomposition"
      ],
      "metadata": {
        "id": "1avgV_R3p8z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lu_decompose(self, A):\n",
        "        \"\"\"\n",
        "        Performs LU Decomposition with partial pivoting (original implementation)\n",
        "        \"\"\"\n",
        "        n = A.shape[0]\n",
        "        L = np.eye(n)\n",
        "        U = A.copy()\n",
        "        P = np.eye(n)\n",
        "\n",
        "        for k in range(n):\n",
        "            # Pivoting\n",
        "            max_row = np.argmax(abs(U[k:, k])) + k\n",
        "            if k != max_row:\n",
        "                U[[k, max_row]] = U[[max_row, k]]\n",
        "                P[[k, max_row]] = P[[max_row, k]]\n",
        "                if k > 0:\n",
        "                    L[[k, max_row], :k] = L[[max_row, k], :k]\n",
        "\n",
        "            # LU Decomposition\n",
        "            for i in range(k + 1, n):\n",
        "                L[i, k] = U[i, k] / U[k, k]\n",
        "                U[i, k:] -= L[i, k] * U[k, k:]\n",
        "\n",
        "        return L, U, P\n",
        "SparkMatrixInversion.lu_decompose = lu_decompose"
      ],
      "metadata": {
        "id": "3Oa1EHlvoF0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Validating Output"
      ],
      "metadata": {
        "id": "hqJ5K5V5quJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MatrixInversion\").getOrCreate()\n",
        "\n",
        "# Create matrix inverter\n",
        "matrix_inverter = SparkMatrixInversion(spark)\n",
        "\n",
        "# Example matrix\n",
        "matrix = np.random.rand(256, 256)\n",
        "# matrix = np.array([[4,7],[2,6]])\n",
        "\n",
        "# Perform block matrix inversion\n",
        "inverse_matrix = matrix_inverter.block_matrix_inverse(matrix)"
      ],
      "metadata": {
        "id": "spjAoD4wUgb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3jfV1QZke3c",
        "outputId": "469734b3-77a3-4c2c-861a-fddecc9d83ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.02071588,  0.14597555, -0.22529224, ...,  0.05853017,\n",
              "        -0.11440219, -0.17311356],\n",
              "       [-0.4661433 ,  0.25643795, -0.34069122, ..., -0.18012414,\n",
              "        -0.02902878, -0.35302252],\n",
              "       [-0.03244434, -0.01204481,  0.08515246, ...,  0.14569533,\n",
              "        -0.00846539,  0.09404938],\n",
              "       ...,\n",
              "       [ 0.50477678,  0.18524246,  0.08792011, ...,  0.02952585,\n",
              "         0.33108323, -0.01340607],\n",
              "       [ 0.18509081,  0.35737958, -0.27436789, ...,  0.18092087,\n",
              "         0.12201275, -0.18678446],\n",
              "       [ 0.03079864,  0.81603672, -0.44965053, ...,  0.70355329,\n",
              "         0.5987302 , -0.60555205]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_matrix @ matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OotlVoPOU3J1",
        "outputId": "e8b46f25-13b4-45a1-aa4f-98abb7841d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.00000000e+00, -9.94329094e-15, -1.78990974e-15, ...,\n",
              "        -1.04014840e-14, -2.02708202e-14, -3.84625941e-14],\n",
              "       [-1.20847785e-15,  1.00000000e+00, -5.15431203e-15, ...,\n",
              "        -1.11177294e-14,  1.06235415e-14,  1.22446251e-14],\n",
              "       [ 1.50765819e-14, -6.87109559e-15,  1.00000000e+00, ...,\n",
              "         4.90777690e-15, -1.18863541e-14,  1.55382757e-14],\n",
              "       ...,\n",
              "       [ 3.98760828e-15,  8.10675697e-15,  3.79170118e-15, ...,\n",
              "         1.00000000e+00,  5.35698944e-15,  3.82557316e-15],\n",
              "       [ 5.93925011e-16, -2.10775642e-15, -6.86868951e-15, ...,\n",
              "        -3.53616991e-15,  1.00000000e+00, -2.59096469e-15],\n",
              "       [ 6.41571543e-15, -2.11505152e-15, -1.18275793e-14, ...,\n",
              "        -8.34700956e-16, -4.81296735e-15,  1.00000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparion with Identity Matrix"
      ],
      "metadata": {
        "id": "_8g_nsuDrZi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I = np.eye(256)\n",
        "difference = np.linalg.norm(inverse_matrix @ matrix - I)\n",
        "tolerance = 1e-10\n",
        "if difference < tolerance:\n",
        "    print(\"The product is almost the identity matrix.\")\n",
        "else:\n",
        "    print(\"The product is not close to the identity matrix.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_61qNOPrNww",
        "outputId": "ed425b7d-09d2-4f05-ee0d-cdd72e1fe01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The product is almost the identity matrix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation Without using spark ( Backbone implementation of Algorithm)"
      ],
      "metadata": {
        "id": "rD8WqUvwrdKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LU Decomposition"
      ],
      "metadata": {
        "id": "eeel4H6Twr5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LU_Decompose(A):\n",
        "    n = A.shape[0]\n",
        "    L = np.eye(n)\n",
        "    U = A.copy()\n",
        "    P = np.eye(n)\n",
        "\n",
        "    for k in range(n):\n",
        "        # Pivoting\n",
        "        max_row = np.argmax(abs(U[k:, k])) + k\n",
        "        if k != max_row:\n",
        "            U[[k, max_row]] = U[[max_row, k]]\n",
        "            P[[k, max_row]] = P[[max_row, k]]\n",
        "            if k > 0:\n",
        "                L[[k, max_row], :k] = L[[max_row, k], :k]\n",
        "\n",
        "        # LU Decomposition\n",
        "        for i in range(k + 1, n):\n",
        "            L[i, k] = U[i, k] / U[k, k]\n",
        "            U[i, k:] -= L[i, k] * U[k, k:]\n",
        "\n",
        "    return L, U, P\n"
      ],
      "metadata": {
        "id": "E5IDltUvwtww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##getLU"
      ],
      "metadata": {
        "id": "QB3rTUA3xCby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getLU(LU_of_M):\n",
        "    L1_inv, S_hat, L3_inv, U1_inv, T, P1M2, U3_inv, P1, P2 = LU_of_M\n",
        "\n",
        "    # Construct the block matrix components for L_inv, U_inv, and P\n",
        "    S = P2 @ S_hat\n",
        "    # L_inv = np.block([[L1_inv, np.zeros_like(S_hat)],\n",
        "    #                   [-np.dot(L3_inv, S_hat), L3_inv]])\n",
        "    L_inv = np.block([[L1_inv, np.zeros((L1_inv.shape[0], L3_inv.shape[1]))],\n",
        "                  [-L3_inv @ S, L3_inv]])\n",
        "\n",
        "\n",
        "    # U_inv = np.block([[U1_inv, -np.dot(T, U3_inv)],\n",
        "    #                   [np.zeros_like(T), U3_inv]])\n",
        "    U_inv = np.block([[U1_inv, -T @ P1M2 @ U3_inv],\n",
        "                  [np.zeros((U3_inv.shape[0], U1_inv.shape[1])), U3_inv]])\n",
        "\n",
        "\n",
        "    P = np.block([[P1, np.zeros_like(P2)],\n",
        "                  [np.zeros_like(P1), P2]])\n",
        "\n",
        "    return L_inv, U_inv, P\n"
      ],
      "metadata": {
        "id": "RGs2VnvuwwQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Block Decomposition Algorithm"
      ],
      "metadata": {
        "id": "grLw4owpxEkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BlockLU_Decompose(M):\n",
        "    n = M.shape[0]\n",
        "    mid = n // 2\n",
        "\n",
        "    M1 = M[:mid, :mid]\n",
        "    M2 = M[:mid, mid:]\n",
        "    M3 = M[mid:, :mid]\n",
        "    M4 = M[mid:, mid:]\n",
        "\n",
        "    M1Mid = M1.shape[0] // 2\n",
        "    if M1Mid <= 64:\n",
        "        L1, U1, P1 = LU_Decompose(M1)\n",
        "        L1_inv = np.linalg.inv(L1)\n",
        "        U1_inv = np.linalg.inv(U1)\n",
        "        T = U1_inv @ L1_inv\n",
        "        S_hat = M3 @ T\n",
        "        M_hat = M4 - S_hat @ (P1 @ M2)\n",
        "        L3, U3, P2 = LU_Decompose(M_hat)\n",
        "        L3_inv = np.linalg.inv(L3)\n",
        "        U3_inv = np.linalg.inv(U3)\n",
        "\n",
        "    else:\n",
        "        LU_of_M1 = BlockLU_Decompose(M1)\n",
        "        L1_inv, U1_inv, P1 = getLU(LU_of_M1)\n",
        "        T = U1_inv @ L1_inv\n",
        "        S_hat = M3 @ T\n",
        "        M_hat = M4 - S_hat @ (P1 @ M2)\n",
        "        LU_of_M_hat = BlockLU_Decompose(M_hat)\n",
        "        L3_inv, U3_inv, P2 = getLU(LU_of_M_hat)\n",
        "    return L1_inv, S_hat, L3_inv, U1_inv, T, P1@M2, U3_inv, P1, P2\n"
      ],
      "metadata": {
        "id": "7xVDWXEywyxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inverse Function that takes matrix as input and returns inverse of the Matrix"
      ],
      "metadata": {
        "id": "v7oy8L0VxHKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BlockInverse(A):\n",
        "  n = A.shape[0]\n",
        "  mid = n//2\n",
        "  if mid <= 64:\n",
        "    return np.linalg.inv(A)\n",
        "  else:\n",
        "    A1 = A[:mid, :mid]\n",
        "    A2 = A[:mid, mid:]\n",
        "    A3 = A[mid:, :mid]\n",
        "    A4 = A[mid:, mid:]\n",
        "\n",
        "    LU_of_A = BlockLU_Decompose(A)\n",
        "    L1_inv, S_hat, L3_inv, U1_inv, T, P1A2, U3_inv, P1, P2 = LU_of_A\n",
        "    X1 = T @ P1\n",
        "    X2 = U3_inv @ L3_inv\n",
        "    Y1 = X1 @ A2\n",
        "    Y2 = X2 @ A3\n",
        "    A_inv_4 = X2\n",
        "    A_inv_3 = -Y2 @ X1\n",
        "    A_inv_2 = -Y1 @ X2\n",
        "    A_inv_1 = X1 - Y1 @ A_inv_3\n",
        "    return np.block([[A_inv_1, A_inv_2], [A_inv_3, A_inv_4]])"
      ],
      "metadata": {
        "id": "zB8TJvcJw1Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Validating Output"
      ],
      "metadata": {
        "id": "kYcrlV0KxPzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.rand(256, 256)\n",
        "I = np.eye(256)\n",
        "product = A @ BlockInverse(A)"
      ],
      "metadata": {
        "id": "Oid2V_ujw5Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2CPADmCw7Us",
        "outputId": "d741dec9-b99b-4fbb-ed6c-e9087ca9a4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.00000000e+00,  1.93498851e-12,  1.74619124e-12, ...,\n",
              "         5.57222365e-14,  4.56480684e-14, -3.04048939e-13],\n",
              "       [ 7.88081180e-14,  1.00000000e+00, -1.03016954e-13, ...,\n",
              "         9.71201291e-14, -1.01485428e-13,  1.05190410e-13],\n",
              "       [ 5.39660790e-14, -1.44066096e-12,  1.00000000e+00, ...,\n",
              "         9.47758045e-14, -1.06860243e-13,  5.19931196e-15],\n",
              "       ...,\n",
              "       [-1.16629713e+00,  2.09706104e+00, -2.08906342e+00, ...,\n",
              "         7.87011081e-14,  1.12711443e-13, -3.69846926e-13],\n",
              "       [ 3.21846719e+00, -1.32779198e+00, -2.67559208e+00, ...,\n",
              "         7.98493088e-14, -6.71843350e-14,  3.56905486e-14],\n",
              "       [-1.30033700e+00,  1.83831897e+00,  1.24131002e+00, ...,\n",
              "         6.85283086e-14, -1.29557734e-14, -2.44953747e-13]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}